train_ratio: 0.8
dev_ratio: 0.1
test_ratio: 0.1
random_state: 42
max_epochs: 100
num_layers_lstm: 1
num_lstm_units: 1024
embedding_dim: 100
batch_size: 64
pad_token: "<PAD>"
num_workers: 4
learning_rate: 0.01
momentum: 0.9
weight_decay: 0.0001
factor: 0.1
patience: 5
threshold: 0.0001
model_directory: "data/model/bipgclstm"
save_model_each_n_epochs: 5