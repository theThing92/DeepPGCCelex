batch_size: 64
dev_ratio: 0.1
embedding_dim: 200
factor: 0.5
learning_rate: 0.1
max_epochs: 100
model_directory: data/model/bipgclstm
momentum: 0.9
num_layers_lstm: 1
num_lstm_units: 1024
num_workers: 4
pad_token: <PAD>
patience: 5
random_state: 42
save_model_each_n_epochs: 5
test_ratio: 0.1
threshold: 0.0001
train_ratio: 0.8
weight_decay: 0.0001
